{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering algorithms\n",
    "\n",
    "In this notebook, we will learn about two kinds of clustering algorithms (k-means and agglomerative clustering), but also about the intuition behind how clustering works. We will also perform clustering on text documents.\n",
    "\n",
    "**Table of Contents**  \n",
    "\n",
    "1. [Preparing Synthetic Data](#sec1)\n",
    "2. [K-Means](#sec2)\n",
    "3. [Agglomerative Clustering](#sec3)\n",
    "4. [Distances used in clustering](#sec4)\n",
    "5. [Clustering text documents with k-means](#sec5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "\n",
    "## 1. Preparing synthetic data\n",
    "\n",
    "We will generate a synthetic data set that has natural clusters and then will use clustering functions \n",
    "to see  how well they work. We are doing this, because otherwise it would be difficult to know whether the agorithms were able to find meaningful clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function that will create the synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look up the documentation for `make_blobs` to find out how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(make_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to call `make_blob`, we can generate a dataset. Notice the importance of the feature `random_state`. That ensures that we can reproduce the values whenever we run the notebook.\n",
    "\n",
    "I want to create data that is groued in four clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=120, \n",
    "                       centers=4,\n",
    "                       cluster_std=0.40, \n",
    "                       random_state=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created the matrix `X` that has 120 rows (one for each sample) with two columns each (meaning two features), and the vector `y_true`, that has 120 values, indicating which cluster (or blob) a data sample (or instance) belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have the same number of values in each blob, by counting unique values with np:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(y_true, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a two-dimensional array, every point has two feature values (so that we can plot it on a 2D plane):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data samples in X to see the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50); # s is the size of the dots in the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty clear that the data points belong to four different clusters. Let's use the various clustering algorithms we have learned about to find out how the data will be clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## 2. K-Means\n",
    "\n",
    "K-means clustering will divide data in k clusters, each with its own center. The algorithm works iteratively  \n",
    "to group together data points that are spatially closer to one-another.\n",
    "\n",
    "We will use the Kmeans algorithm that is implemented within the `sklearn` package.\n",
    "\n",
    "All `sklearn` algorithms have the same API involving these steps:\n",
    "\n",
    "1. initialize the model\n",
    "2. fit the data to the model\n",
    "3. validate the model or predict new values\n",
    "\n",
    "Each of these steps is usually one single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "\n",
    "kmeans = KMeans(n_clusters=4)  # step 1: initialize the model\n",
    "kmeans.fit(X)                  # step 2: fit the data to the model\n",
    "y_kmeans = kmeans.predict(X)   # step 3: prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results in order to see whether the clustering worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kmeans(kmeans, y_kmeans):\n",
    "    \"\"\"Helper function to plot the results of the kmeans clustering\"\"\"\n",
    "    # Plot the points\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.scatter(X[:, 0], # first column \n",
    "                X[:, 1], # second column\n",
    "                c=y_kmeans, # indices for the color mapping\n",
    "                s=50,  # size of a dot\n",
    "                cmap='plasma') # color palette\n",
    "    \n",
    "    # Plot the cluster centers\n",
    "    centers = kmeans.cluster_centers_\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_kmeans(kmeans, y_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable center stores the points that are the centroids of the clusters\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up the coordinates of the centers in the plot and identify the clusters (first row is cluster 0, second row is cluster 1, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tighter data\n",
    "We can play with the parameters of our data generation process to create different-looking clusters; for example clusters that are not well separated, by increasing the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=300, centers=5,\n",
    "                       cluster_std=0.8, # notice this is the double of std from the first example\n",
    "                       random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50); # s is the size of the dots in the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how good the k-means clustering algorithm will perform this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=5) \n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Do visualization\n",
    "plot_kmeans(kmeans, y_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we ask it to find 2 clusters, instead of the 5 natural clusters that are in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=2) \n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Do visualization\n",
    "plot_kmeans(kmeans, y_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask about 3 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfom Clustering\n",
    "kmeans = KMeans(n_clusters=3) \n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Do visualization\n",
    "plot_kmeans(kmeans, y_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agorithm does the right thing even when we ask to find less clusters, thus, it's up to us to decide how many we want. For that, we can use the \"Elbow Point\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Elbow Point\n",
    "\n",
    "The K-means algorithm tries to find centroids that minimise the **inertia**, which is the within-cluster sum-of-squares criterion. \n",
    "\n",
    "That is, for every point x, we find the square of the distance from its cluster's centroid and take the overall sums of all points in the cluster. The smaller this sum, the tighter the cluster, the better the clustering quality. The optimal number of clusters is where this error stops decreasin, which in a graph is shown as an **elbow point**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_viz(X, num):\n",
    "\n",
    "    # Perform clustering from 1 to num\n",
    "    cost =[] \n",
    "    for i in range(1, num): \n",
    "        KM = KMeans(n_clusters = i) \n",
    "        KM.fit(X) \n",
    "          \n",
    "        # calculates squared error for the clustered points \n",
    "        cost.append(KM.inertia_)      \n",
    "      \n",
    "    # plot the cost against K values \n",
    "    plt.plot(range(1, num), cost, color ='g', linewidth ='3') \n",
    "    plt.xlabel(\"Value of K\") \n",
    "    plt.ylabel(\"Squared Error (Cost)\") \n",
    "    plt.show() \n",
    "\n",
    "# Call function\n",
    "elbow_viz(X, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitively see a sharp decrease at k=2 (our data has the best separation at 2 clusers). However, we can also see that the inertia continues decreasing and it stops doing so at k=5 (between 4 and 6). 5 is the natural number of clusters in this synthetic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorations\n",
    "\n",
    "Let's explore the Elbow method a bit more. \n",
    "\n",
    "**Create three clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "X, y_true = make_blobs(n_samples=150, \n",
    "                       centers=3,\n",
    "                       cluster_std=0.40, \n",
    "                       random_state=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_viz(X, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this occasion, the number k=3 is very clear from the Elbow visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create 7 clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "X, y_true = make_blobs(n_samples=210, \n",
    "                       centers=7,\n",
    "                       cluster_std=0.30, \n",
    "                       random_state=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_viz(X, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>\n",
    "## 3. Agglomerative Clustering\n",
    "\n",
    "In Kmeans clustering, we provide the number of clusters and then the algorithm partitions the data, so that they are clustered around a center point. \n",
    "In agglomerative clustering, the data points are grouped together based on distance from one another, and we can decide how many clusters we want, once we see how data are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# linkage is the the function that performs the clustering\n",
    "Z = linkage(X, \n",
    "            method='single', \n",
    "            metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create string labels for the data samples, to show in the dendrogram\n",
    "ticks = [f\"el_{i}_group={el}\" for i, el in enumerate(y_true)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the dendrogram (takes a few seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_dendrogram(Z, ticks):\n",
    "    \"\"\"Helper function to draw a dendrogram for testing purposes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.title('Hierarchical Clustering')\n",
    "    plt.xlabel('samples')\n",
    "    plt.ylabel('distance')\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=8.,  # font size for the x axis labels\n",
    "        labels = ticks\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "draw_dendrogram(Z, ticks)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something this makes clear is that visualizing the dendrogram is useful, but not that useful when we have many points. The dataset in the chart has a lot points, and it's currently hard to see the labels of the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a smaller dataset\n",
    "Let's create a smaller dataset that also is a bit more separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=80, centers=3,\n",
    "                       cluster_std=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the hierarchical clustering again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(X, method='single', metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now plot the dendrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = ['el_{}_group={}'.format(i, el) for i, el in enumerate(y_true)]\n",
    "\n",
    "draw_dendrogram(Z, ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, even the labels are visible. In fact, we can see how the original groups (notice the second part of name) are clustered together. The first cluster (left) contains points from group=1, the middle cluster contains points from group=0, and the cluster on the right contains points from group=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=60, centers=4,\n",
    "                       cluster_std=0.3, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(X, method='single', metric='euclidean')\n",
    "\n",
    "ticks = ['el_{}_group={}'.format(i, el) for i, el in enumerate(y_true)]\n",
    "\n",
    "draw_dendrogram(Z, ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec4\"></a>\n",
    "## 4. Distance metrics used in clustering\n",
    "\n",
    "Clustering algorithms use various \"distance metrics\" to measure how far apart two points are in space. \n",
    "\n",
    "In `scipy` we can calculate the **pairwise distance** of every two data points with the function `pdist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "X, y_true = make_blobs(n_samples=80, centers=3,\n",
    "                       cluster_std=0.4, random_state=0)\n",
    "\n",
    "xdist = pdist(X, 'euclidean')\n",
    "xdist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why is the shape of the distance vector 3160? The dataset X has 80 points. What formula involving the data size n=80 will produce the result 3160? \n",
    "\n",
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look up the first 5 values of X first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look up the first 5 values of xdist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdist[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value in the xdist is the distance between points X[0] and X[1], that distance is small, because, as you can see below the values for X[0] and X[1], they are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0], X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the second value, 3.97438162, means that X[0] and X[2] are further apart, see values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0], X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this even better if we do a scatterplot of only these five points and label the points with their index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array with five points\n",
    "X5 = X[:5]\n",
    "\n",
    "# make scatterplot\n",
    "plt.scatter(X5[:, 0], X5[:, 1], s=50)\n",
    "\n",
    "# show text next to each point (notice the delta-s, to be slightly off the point)\n",
    "for i in range(5):\n",
    "    plt.text(X5[i,0]-0.05, X5[i,1]-0.05, s=f\"{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows clearly that the point 0 is very close to points 1 and 4 and somewhat to point 3, but is further away from point 2. As a reminder, these four distances (of the point 0 to the other 4 points) are captured by the first elements of xdist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdist[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More distance metrics**\n",
    "\n",
    "We only showed here the euclidean distance, but the function `pdist`, short for pairwise distance, has implementations of more than 20 different distance metrics, they are listed [on this page](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html). \n",
    "\n",
    "**Distance matrix**\n",
    "\n",
    "Once we create a vector of pairwise distances with `pdist`, we can display them as a matrix, to create the so-called distance matrix, where we can look up the distance between any two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "distM = squareform(xdist) # make a matrix\n",
    "distM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distance matrix can be visualiazed through a heatmap. We can easily build one with seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(distM, cmap=\"Purples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec5\"></a>\n",
    "## 5. Clustering documents with k-means\n",
    "\n",
    "We'll try clustering with a famous text dataset called 20newsgroups. Read a bit about its content on [this page](http://qwone.com/~jason/20Newsgroups/).\n",
    "\n",
    "Because this is a famous dataset, `sklearn` already knows how to read its content from files. If you are curious about the files, download the ZIP archive from the website. Careful, it's a big file.\n",
    "\n",
    "First, sklearn will get the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "all_data = sklearn.datasets.fetch_20newsgroups(subset='all')\n",
    "print(len(all_data.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains 18846 files in total, so, it's a big dataset.  \n",
    "\n",
    "Documents are grouped in classes (a class is known as a target). Let's see these classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this dataset is usually used for text classification, it is divided in two parts: \"train\" data and \"test\" data. Both of these groups have examples with labels (the category where a piece of news belongs), but the algorithm will learn its model from the train data and then it is tested for accuracy (how well is doing) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train')\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test')\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, 60% of files are assigned to training and 40% are assigned to testing.\n",
    "\n",
    "One doesn't need to work with all 20 groups at once, we can choose to focus on a subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['alt.atheism', 'comp.graphics', 'rec.autos', \n",
    "          'rec.sport.baseball', 'sci.med', 'talk.politics.guns']\n",
    "\n",
    "\n",
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train', categories=groups)\n",
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test', categories=groups)\n",
    "print(len(train_data.filenames), len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing documents as vectors\n",
    "\n",
    "Clustering works with vectors of numbers, thus, all documents need to be converted into such vectors. This is what we had to do in Assignment 6 as well. This process is known as vectorizing, and we used some vectorizers in Assignment 6. However, our features there were simple. When we are working with free text, the process needs to be more involved. Below we show a vectorizer that does two things together:\n",
    "\n",
    "1. stemming of words (reducing a word to its stem, for example: \"working\"--> \"work\", etc.), \n",
    "2. calculation of tf*idf (the product of term frequency and the inverse document frequency) (for the moment, don't worry what these are, we'll discuss them in class briefly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choices**  \n",
    "In addition to removing stop-words, getting stems, ignoring unicode characters that cannot be decoded, we are also dropping all words that appear less than 10 times (this will remove spelling mistakes or unusual acronyms), as well as words that appear in 50% of the documents. All these efforts are for the goal of reducing the dimensions of the problem (by having a smaller number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5, \n",
    "                                    stop_words='english', \n",
    "                                    decode_error='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the data to this vectorizer, it might take a few seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- there are 3395 rows in the matrix, each of them representing a document (an email)\n",
    "- there are 5068 columns in the matrix, each of them representing a feature (a stemmed word)\n",
    "- such a matrix would have 17,205,860 cells, if not in sparse format. However, it's clear that each document will have only a few dozen words, thus most of the values in each row are 0. In fact, there is a toal of 272,628 non-zero values in this matrix.\n",
    "- each non-zero cell contain the tf*idf value of the feature in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use attributes and methods of the vectorizer to access more information about the model we have built. For example, we can look at the words (or the vocabulary of the corpus):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable is a dictionary, so we can loop up some of its keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vectorizer.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words in the vocabulary are the feature names, we can verify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()[2730:2745]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Clustering\n",
    "\n",
    "Lets initialize the clustering algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 12\n",
    "km = KMeans(n_clusters=num_clusters, \n",
    "            init='random', \n",
    "            n_init=3, \n",
    "            verbose=1, \n",
    "            random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit the model to the data, meaning, we'll find the parameters for the 12 clusters (coordinates of the centroids). I set **verbose=1**, so that we can see the process of iteration in order to converge to cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `labels_` will indicate for every document the number of cluster to which the document has been assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array tells us that the first document was assigned to cluster 26, the second document to cluster 12, the third document to cluster 25, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that there is a label assigned to every document, by printing the lengths of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at all centers, there are large vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we have 12 centers, each a vector of 5068 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out one centroid, it should be a long array of numbers (tf-idf values) for different features(words) that are relevant to the cluster. Values 0 mean that the feature is not present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(km.cluster_centers_[0])[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many documents are in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "labelsCnt = Counter(km.labels_)\n",
    "\n",
    "for k in sorted(labelsCnt):\n",
    "    print(k, labelsCnt[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seem to be spread out across all clusters, but some clusters are much bigger than others, for example, cluster 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cluster stores the indices of the documents assigned to a cluster. Let's get the indices for one cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will find the document indices that were assigned the label 1\n",
    "\n",
    "indices = [i for i in range(len(km.labels_)) if km.labels_[i] == 1]\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indices allow us to access the documents and their labels, for example, here is how to get the text of the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data[indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how to get the that target label and its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.target[indices[0]], train_data.target_names[train_data.target[indices[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this relationship between target and target_names, it is possible for us to look at the original labels of all documents in Cluster 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetNames = [train_data.target_names[train_data.target[docID]] for docID in indices]\n",
    "Counter(targetNames).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this cluster, there are 50 emails from 'rec.autos', 22 emails from 'comp.graphics' and 9 emails from 'sci.med'. We will do this for all the clusters, let's checks where there instances are coming from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labelsCnt:\n",
    "    indices = [i for i in range(len(km.labels_)) if km.labels_[i] == label]\n",
    "    targetNames = [train_data.target_names[train_data.target[docID]] for docID in indices]\n",
    "    print(f\"Cluster: {label}\")\n",
    "    print(Counter(targetNames).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of these clusters are really good, because they contain emails from a single newgroup (such as baseball) or the majority of documents from one newsgroup. Meanwhile, a few other clusters have documents from several newsgroups. This indicates that we might need to increase the number of clusters to get clusters that are more homogenous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** This example shows that even when working with messy text documents (such as emails), it's possible to perform clustering and find meaningful clusters. Remember, when we did the clustering, we did not provide any labels to the model, this is unsupervised learning. Thus, the groups that we see are automatically coming from the similarity of the documents. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
