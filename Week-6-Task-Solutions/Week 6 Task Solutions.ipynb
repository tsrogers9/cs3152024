{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644e74d1-7750-4f87-9758-b2a4d369ddd9",
   "metadata": {},
   "source": [
    "# Week 6 Tasks: NYT API Data and Cosine Similarity for Text \n",
    "\n",
    "These tasks were discussed during week 6 and you had to work on them with your group.  \n",
    "Below are the solutions and additional tasks that will be helpful for your project.\n",
    "\n",
    "**Table of Content**\n",
    "* [Part 1: Working with the NYT API](#sec1)\n",
    "* [Part 2: Cosine Similarity for Text](#sec2)\n",
    "* [Part 3: Similarity of Spring and Summer sentences](#sec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47288b9e-1d91-4df6-8c98-01d8082c6172",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "## Part 1: Working with the NYT API \n",
    "\n",
    "We have the following goals:\n",
    "\n",
    "1. Use the API to get all articles from a month\n",
    "2. Verify the number of articles\n",
    "3. Find the distribution of articles by section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5235f-e011-42c5-afcd-4678c615fcf1",
   "metadata": {},
   "source": [
    "### Important: replace the string below with your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eded646-a519-4488-9efb-dd5e68c1d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAPIkey = \"dAgeA3Jd143Ih1V0Nc7ljDmJwWyo7C6A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d637cf-7eac-45b1-b6c9-a3c28dd06813",
   "metadata": {},
   "source": [
    "We will write a function that given a date (month and year) will talk to the NYT API and get the articles for that time period. We will store the results in a JSON file to process when needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977e6acc-2c29-43ec-8f9a-99de35b5eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "def getNYTArticles(year, month, apiKey):\n",
    "    \"\"\"Function that sends a request to the NYT API for all articles in a month\n",
    "    and then stores the results in a JSON file.\n",
    "    \"\"\"\n",
    "    # create URL\n",
    "    URL = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={apiKey}\"\n",
    "\n",
    "    # send the request to get the data\n",
    "    data = requests.get(URL)\n",
    "    if data.status_code == 200:\n",
    "        print(\"Successfully got the data.\")\n",
    "\n",
    "    dataJson = data.json() # get response as JSON\n",
    "\n",
    "    with open(f\"NYT_{year}-{month}.json\", 'w') as fout:\n",
    "        json.dump(dataJson, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5a98c-0641-4336-8d34-35c626579eab",
   "metadata": {},
   "source": [
    "Let's test the function for the months of February 2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef9ff83-df33-4037-ad86-b029bd53116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully got the data.\n"
     ]
    }
   ],
   "source": [
    "getNYTArticles(2024, 2, myAPIkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1d927-2713-4a74-9d2d-05cfde14e136",
   "metadata": {},
   "source": [
    "## Explore the NYT Data\n",
    "\n",
    "Now that we have the data, we will look into how to retrieve things like article title, section, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d2c5c4-696d-4518-a513-443d3db5fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"NYT_2024-2.json\") as fin:\n",
    "    articles = json.load(fin)\n",
    "\n",
    "print(type(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe47492-d11a-45d4-b1a9-3ebf1aeed9cf",
   "metadata": {},
   "source": [
    "We can check the keys of this dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32c6639-9d09-47e0-8299-b1733379e50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['copyright', 'response'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414a10a-b15a-44b4-805f-70877b0f6c3f",
   "metadata": {},
   "source": [
    "Then we check what values are stored under each key, without printing the values, but checking for their type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c542d6-248d-40aa-a580-443f4b68aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyright <class 'str'>\n",
      "response <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for key in articles:\n",
    "    print(key, type(articles[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133b5d15-0bac-47c0-8acd-57a4e3d0f659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copyright (c) 2024 The New York Times Company. All Rights Reserved.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['copyright']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7ac7a-003a-4413-ad3d-aaaa4f893c24",
   "metadata": {},
   "source": [
    "Let's look at the keys for 'response':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f21c0e-0fd0-45b1-b80b-d82cd6672472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['docs', 'meta'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['response'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d43299-30db-404b-a80d-1ed2c75fc453",
   "metadata": {},
   "source": [
    "One more time, we look what kind of information is stored under each of these keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8affb171-a949-4507-928d-bde3c03ca065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs <class 'list'>\n",
      "meta <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for key in articles['response']:\n",
    "    print(key, type(articles['response'][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13648859-3869-4274-98ca-4ed39e8ebc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hits': 3791}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is under the \"meta\" key?\n",
    "\n",
    "articles['response']['meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac6aee-0f8a-4ac2-a51d-eb57cad8664d",
   "metadata": {},
   "source": [
    "So, this shows how many articles are in the data. We can verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f30f73-eb9e-4865-801d-54fb7452eba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3791"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles['response']['docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b437bb-dc0d-4162-8369-ce547030d873",
   "metadata": {},
   "source": [
    "It's the same number, which is a good thing. Now let's look at what one of the articles (or docs) looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "085e6715-f6ec-4d9a-bdd6-23cb86fee3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'Periods of backlash take shape after surges of Black progress. We have entered another such period.',\n",
       " 'web_url': 'https://www.nytimes.com/2024/01/31/opinion/racist-backlash-history.html',\n",
       " 'snippet': 'Periods of backlash take shape after surges of Black progress. We have entered another such period.',\n",
       " 'lead_paragraph': 'I am fascinated, and alarmed, by the swiftness with which periods of backlash take shape after surges of Black progress, and I believe that we have entered another such period.',\n",
       " 'print_section': 'A',\n",
       " 'print_page': '21',\n",
       " 'source': 'The New York Times',\n",
       " 'multimedia': [{'rank': 0,\n",
       "   'subtype': 'xlarge',\n",
       "   'caption': None,\n",
       "   'credit': None,\n",
       "   'type': 'image',\n",
       "   'url': 'images/2024/02/01/multimedia/31blow1-hvfq/31blow1-hvfq-articleLarge.jpg',\n",
       "   'height': 800,\n",
       "   'width': 600,\n",
       "   'subType': 'xlarge',\n",
       "   'crop_name': 'articleLarge',\n",
       "   'legacy': {'xlarge': 'images/2024/02/01/multimedia/31blow1-hvfq/31blow1-hvfq-articleLarge.jpg',\n",
       "    'xlargewidth': 600,\n",
       "    'xlargeheight': 800}},\n",
       "  {'rank': 0,\n",
       "   'subtype': 'jumbo',\n",
       "   'caption': None,\n",
       "   'credit': None,\n",
       "   'type': 'image',\n",
       "   'url': 'images/2024/02/01/multimedia/31blow1-hvfq/31blow1-hvfq-jumbo.jpg',\n",
       "   'height': 1024,\n",
       "   'width': 768,\n",
       "   'subType': 'jumbo',\n",
       "   'crop_name': 'jumbo',\n",
       "   'legacy': {}},\n",
       "  {'rank': 0,\n",
       "   'subtype': 'superJumbo',\n",
       "   'caption': None,\n",
       "   'credit': None,\n",
       "   'type': 'image',\n",
       "   'url': 'images/2024/02/01/multimedia/31blow1-hvfq/31blow1-hvfq-superJumbo.jpg',\n",
       "   'height': 2048,\n",
       "   'width': 1536,\n",
       "   'subType': 'superJumbo',\n",
       "   'crop_name': 'superJumbo',\n",
       "   'legacy': {}},\n",
       "  {'rank': 0,\n",
       "   'subtype': 'thumbnail',\n",
       "   'caption': None,\n",
       "   'credit': None,\n",
       "   'type': 'image',\n",
       "   'url': 'images/2024/02/01/multimedia/31blow1-hvfq/31blow1-hvfq-thumbStandard.jpg',\n",
       "   'height': 75,\n",
       "   'width': 75,\n",
       "   'subType': 'thumbnail',\n",
       "   'crop_name': 'thumbStandard',\n",
       "   'legacy': {'thumbnail': 'images/2024/02/01/multimedia/31blow1-hvfq/31blow1-hvfq-thumbStandard.jpg',\n",
       "    'thumbnailwidth': 75,\n",
       "    'thumbnailheight': 75}},\n",
       "  {'rank': 0,\n",
       "   'subtype': 'thumbLarge',\n",
       "   'caption': None,\n",
       "   'credit': None,\n",
       "   'type': 'image',\n",
       "   'url': 'images/2024/02/01/multimedia/31blow1-hvfq/31blow1-hvfq-thumbLarge.jpg',\n",
       "   'height': 150,\n",
       "   'width': 150,\n",
       "   'subType': 'thumbLarge',\n",
       "   'crop_name': 'thumbLarge',\n",
       "   'legacy': {}}],\n",
       " 'headline': {'main': 'The Dawn of a New Era of Oppression',\n",
       "  'kicker': 'Charles M. Blow',\n",
       "  'content_kicker': None,\n",
       "  'print_headline': 'The Dawn of a New Era of Oppression',\n",
       "  'name': None,\n",
       "  'seo': None,\n",
       "  'sub': None},\n",
       " 'keywords': [{'name': 'subject',\n",
       "   'value': 'Hate Crimes',\n",
       "   'rank': 1,\n",
       "   'major': 'N'},\n",
       "  {'name': 'subject', 'value': 'Black People', 'rank': 2, 'major': 'N'},\n",
       "  {'name': 'subject', 'value': 'Blacks', 'rank': 3, 'major': 'N'},\n",
       "  {'name': 'subject', 'value': 'Discrimination', 'rank': 4, 'major': 'N'},\n",
       "  {'name': 'subject',\n",
       "   'value': 'Civil Rights Movement (1954-68)',\n",
       "   'rank': 5,\n",
       "   'major': 'N'},\n",
       "  {'name': 'subject', 'value': 'Reconstruction Era', 'rank': 6, 'major': 'N'},\n",
       "  {'name': 'subject',\n",
       "   'value': 'Segregation and Desegregation',\n",
       "   'rank': 7,\n",
       "   'major': 'N'},\n",
       "  {'name': 'persons',\n",
       "   'value': 'Nixon, Richard Milhous',\n",
       "   'rank': 8,\n",
       "   'major': 'N'}],\n",
       " 'pub_date': '2024-02-01T00:00:08+0000',\n",
       " 'document_type': 'article',\n",
       " 'news_desk': 'OpEd',\n",
       " 'section_name': 'Opinion',\n",
       " 'byline': {'original': 'By Charles M. Blow',\n",
       "  'person': [{'firstname': 'Charles',\n",
       "    'middlename': None,\n",
       "    'lastname': '',\n",
       "    'qualifier': None,\n",
       "    'title': None,\n",
       "    'role': 'reported',\n",
       "    'organization': '',\n",
       "    'rank': 1}],\n",
       "  'organization': None},\n",
       " 'type_of_material': 'Op-Ed',\n",
       " '_id': 'nyt://article/f2a4bafd-6333-5aee-8bd7-08fa9572ec5e',\n",
       " 'word_count': 928,\n",
       " 'uri': 'nyt://article/f2a4bafd-6333-5aee-8bd7-08fa9572ec5e'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['response']['docs'][0] # using indexing, because we know that the data is stored in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50978e8-b3aa-41c5-b549-4b4f99941214",
   "metadata": {},
   "source": [
    "We can see tha an article is a somewhat nested data structure, it's a dictionary, but many of the keys point to list of other dictionaries. Let's look at the top fields: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c0c987-974d-4e76-aa5e-afb05fa33ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract <class 'str'>\n",
      "web_url <class 'str'>\n",
      "snippet <class 'str'>\n",
      "lead_paragraph <class 'str'>\n",
      "print_section <class 'str'>\n",
      "print_page <class 'str'>\n",
      "source <class 'str'>\n",
      "multimedia <class 'list'>\n",
      "headline <class 'dict'>\n",
      "keywords <class 'list'>\n",
      "pub_date <class 'str'>\n",
      "document_type <class 'str'>\n",
      "news_desk <class 'str'>\n",
      "section_name <class 'str'>\n",
      "byline <class 'dict'>\n",
      "type_of_material <class 'str'>\n",
      "_id <class 'str'>\n",
      "word_count <class 'int'>\n",
      "uri <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "oneArticle = articles['response']['docs'][0]\n",
    "for key in oneArticle:\n",
    "    print(key, type(oneArticle[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92a84392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Dawn of a New Era of Oppression'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['response']['docs'][0]['headline']['main']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc22ee-0748-4850-bc23-272d0dbc6340",
   "metadata": {},
   "source": [
    "### Find the distribution of articles by section\n",
    "\n",
    "As we saw above, every article has a section name, so we can easily collect all those names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83e0f050-8a06-45a6-b33d-b6b2a485abc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Opinion', 'New York', 'Opinion', 'World', 'U.S.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = [article['section_name'] for article in articles['response']['docs']]\n",
    "\n",
    "# Let's look up a few of them\n",
    "sections[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9890db0b-3c4f-46a1-8338-b31a4cf4acb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.', 734),\n",
       " ('World', 513),\n",
       " ('Arts', 326),\n",
       " ('Opinion', 272),\n",
       " ('Business Day', 244),\n",
       " ('New York', 200),\n",
       " ('Style', 174),\n",
       " ('Books', 139),\n",
       " ('Crosswords & Games', 125),\n",
       " ('Movies', 123)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "distDct = Counter(sections) # count the occurrences of each section name\n",
    "\n",
    "distDct.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383daa6-b200-45dd-a49f-fdf15f040518",
   "metadata": {},
   "source": [
    "## Tasks for you:\n",
    "\n",
    "1. [Write a Python function that takes a date, for example, \"2024-02-12\", and returns the list of articles for that day.](#1)\n",
    "2. [Write some code that explores whether the fields \"abstract\" and \"snippet\" are always the same or they differ. Which one has more information?](#2)\n",
    "3. [Write a function that given one article (in its nested structure), creates a flat dictionary with keys that are relevant for analysis: either the abstract or snippet (see point 2); lead paragraph; headline; keywords concatenated via semicolon; pub_date; document_type; section_name; and type_of_material](#3)\n",
    "4. [Write another function that calls the function from point 3 on every article, to create a list of article dictionaries, and convert this list into a dataframe and then store it as a CSV file with the date-month in the title (this is important for point 5 below).](#4)\n",
    "5. [Once you have done all of these in the notebook, create a Python script that can be called with a date (from a TikTok video). First, the script looks whether a CSV with cleaned articles is in our folder. If not, calls first the API function to get the articles and then the function that converts them into a CSV. Then, it loads the CSV into a datafram and it uses filtering to get the articles for the desired date. These articles will be used for the Semantic Similarity portion of the TikTok Project.](#5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "549d3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa0480",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\"></a>\n",
    "#### Task 1\n",
    "Write a Python function that takes a date, for example, \"2024-02-12\", and returns the list of articles for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cb2581b-de88-4e84-8a1c-cb0287025661",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 1\n",
    "def daily_article_list(date):\n",
    "    cwd = os.getcwd()\n",
    "    print(f\"Searching for NYT articles from {date}\")\n",
    "\n",
    "    year = date[:4]\n",
    "    month = date[5:7].strip('0')\n",
    "\n",
    "    month_json_want = f\"NYT_{year}-{month}.json\"\n",
    "    month_json_current = [file for file in os.listdir(cwd) if file.endswith(\".json\")] # check if that month's json is in the folder\n",
    "\n",
    "    if month_json_want not in month_json_current: # if that month's json is not in the folder, get it\n",
    "        print(f\"Getting articles from {month}-{year}\")\n",
    "        getNYTArticles(year, month, myAPIkey)\n",
    "    else:\n",
    "        print(f\"Already have articles from {month}-{year}\")\n",
    "\n",
    "    with open(os.path.join(cwd,month_json_want)) as inf:\n",
    "        articles = json.load(inf)\n",
    "    daily_article_list = [article['headline']['main'] for article in articles['response']['docs'] if article['pub_date'][:10] == date]\n",
    "\n",
    "    return daily_article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62693f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for NYT articles from 2024-02-12\n",
      "Already have articles from 2-2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ad Nods to Taylor Swift and Football, Drawing Cheers and Criticism',\n",
       " 'Trump Says It Would Be ‘Disloyal’ for Taylor Swift to Endorse Biden',\n",
       " 'Usher Brings Precise Details to Pop’s Biggest Stage: The Super Bowl',\n",
       " 'Beyoncé Announces New Album in Super Bowl Commercial',\n",
       " 'Former W.W.F. Wrestler Arrested in Wife’s Murder',\n",
       " 'R.F.K. Jr. Apologizes to His Family for an Allied Group’s Super Bowl Ad',\n",
       " 'What to Know About the 2 Hostages Israel Rescued From Rafah',\n",
       " 'G.O.P. Officials, Once Critical, Stand by Trump After NATO Comments',\n",
       " 'Wildfire Smoke Will Worsen, New Study Shows, and Protections Are Few',\n",
       " 'How China Built BYD, Its Tesla Killer',\n",
       " 'No Corrections: Feb. 12, 2024',\n",
       " 'What to Know About Indonesia’s Election',\n",
       " 'Quotation of the Day: A New Strategy: Wooing the Influencers',\n",
       " 'Monday Briefing',\n",
       " 'The Sun Is Setting on Indonesia’s Democratic Era',\n",
       " 'What’s on TV This Week: ‘True Detective: Night Country’ and ‘Doctor Zhivago’',\n",
       " 'Death Masks from Ancient Egypt Find an Afterlife',\n",
       " 'Word of the Day: adept',\n",
       " 'Biden and Jordan’s King Call on Israel to Protect Palestinians in Rafah',\n",
       " 'Today’s Wordle Review',\n",
       " 'The Connections Companion',\n",
       " 'Spelling Bee Forum',\n",
       " 'When the Voice You Hear Is Not the Actor You See',\n",
       " 'In the Land of George Santos, Machine Politics Fuels a G.O.P. Revival',\n",
       " 'Gallery Shows Last Works by Chuck Close. Will it Repair a Reputation?',\n",
       " 'It’s Mardi Gras. Welcome to The King Cake Drive-Thru.',\n",
       " 'Two Cases. Two Judges. One High-Stakes Week for Trump.',\n",
       " 'She Survived an Airstrike That Killed Her Entire Family in Gaza',\n",
       " 'Should You Ever Cook Dinner on a First Date?',\n",
       " 'Lesbian Chic, for All',\n",
       " 'Light, Pretty Knits for Spring',\n",
       " 'How Female Chefs Are Sparkling in Istanbul',\n",
       " 'Kelly Link Returns with a Dreamlike, Profoundly Beautiful Novel',\n",
       " 'Why We’re Living in an Age of Twins',\n",
       " 'When the Breakup Ruins Your Favorite Restaurant',\n",
       " '$3.7 Million Homes in California',\n",
       " 'What a Split in Consumer Confidence Means for Biden',\n",
       " 'The Special Election to Fill George Santos’s Seat',\n",
       " 'The Deadly Business of Restricting Immigration',\n",
       " 'How GKids Became the A24 of Animation',\n",
       " 'The Deep Joy of Squirreling It All Away',\n",
       " 'Francesca Sloane, Creator of ‘Mr. & Mrs. Smith,’ Finds Poetry in Oddity',\n",
       " 'How to Keep Your Teeth Strong',\n",
       " 'Quietly, After a $4 Million Fee, MoMA Returns a Chagall With a Nazi Taint',\n",
       " 'No Deposits This Year at Love Bank, a Museum of Affection Hit by Fire',\n",
       " 'It’s a Birkin! No, a Dior. No, a Balenciaga. What in the World is It?',\n",
       " 'Should Parents Ever Be Held Responsible for the Harmful Actions of Their Children?',\n",
       " 'Group Focused on Child Care Sets $40 Million Effort to Help Democrats',\n",
       " 'What Was The Village Voice?',\n",
       " 'How Can I Update My Millennial Style?',\n",
       " 'I’m a Neuroscientist. We’re Thinking About Biden’s Memory and Age in the Wrong Way.',\n",
       " 'Hours After Trump Argument, Cautious Reflections From Justice Kagan',\n",
       " 'This Is Not the Way Team Biden Drew It Up',\n",
       " 'I Love You, but I Hate Your Cooking',\n",
       " 'We Keep Domestic Violence Shelters Secret. Who Is That Really Helping?',\n",
       " 'Israel Strikes Rafah, and a Super Bowl Surprise',\n",
       " 'Why Boeing’s Top Airplanes Keep Failing',\n",
       " 'Two Big Texas Oil Producers Announce $26 Billion Merger',\n",
       " 'Nor’easter Expected to Bring New York City’s Heaviest Snow in Over 2 Years',\n",
       " 'The Decline of the N.R.A.',\n",
       " 'Palestinians in Rafah Describe ‘Night Full of Horror’ During Israeli Hostage Rescue',\n",
       " 'The Drama of Sports Transcends the Super Bowl Spectacle',\n",
       " 'The Doomsday Clock Keeps Ticking',\n",
       " 'How Big Was the Big Game?',\n",
       " 'At the Super Bowl, Taylor Swift Gives a Fashion Week Brand a Boost',\n",
       " 'The 20th Anniversary of California’s First Same-Sex Marriages',\n",
       " 'Pope and Argentine President Appear to Find Some Common Ground',\n",
       " 'A Mushroom Grew in a Strange Place: The Side of a Frog',\n",
       " 'Trump Asks Supreme Court to Pause Ruling Denying Him Absolute Immunity',\n",
       " 'Dutch Court Moves to Block Export of Fighter Jet Parts to Israel',\n",
       " 'Leaving Las Vegas to High Rollers, Some 49ers Fans Chose Reno',\n",
       " 'Can Exercise Help Prevent Prostate Cancer?',\n",
       " 'Nothing Says ‘Be Mine’ Like a Chocolate Chip Cookie the Size of Your Face',\n",
       " 'Kali Malone Studied Farming. Fate Brought Her to Avant-Garde Music.',\n",
       " 'Biden’s First TikTok Post Jokes About Super Bowl Conspiracy Theory',\n",
       " 'Brazilian Police Seek Husband in Murder of Noted Art Dealer',\n",
       " 'New York City Public Schools Will Hold Remote Classes Because of Snow',\n",
       " 'Can You Name the Sequels to These Best-Selling Novels?',\n",
       " 'Twyla Tharp: ‘You Dig Down, You Settle in, You Don’t Stop’',\n",
       " 'The Cookbook ‘French Boulangerie’ Embraces Fermentation and Goes Beyond France',\n",
       " 'Finland’s New President Faces Unexpected First Test: Not Russia, but Trump',\n",
       " 'Biden’s Age as a Major Campaign Issue',\n",
       " 'Houston Megachurch Shooter Had an AR-15 and Brought Her 7-Year-Old Son',\n",
       " 'Who Are the Major Players After Pakistan’s Stunning Election?',\n",
       " 'Cecilia Gentili, Transgender Activist, Performer and Author, Dies at 52',\n",
       " 'CoStar Group’s Super Bowl Debut: Dan Levy, Heidi Gardner and Jeff Goldblum',\n",
       " 'Congress to Examine U.S. Spy Agencies’ Work on Havana Syndrome',\n",
       " '‘An Incoherent Riot’: Why London’s Skyline Looks So Weird',\n",
       " 'An Asteroid Wiped Out Dinosaurs. Did It Help Birds Flourish?',\n",
       " 'Georgia Judge Will Hear Evidence on Relationship Between Trump Prosecutors',\n",
       " 'Why I Am Now Deeply Worried for America',\n",
       " 'A Left-vs.-Left House Battle, Funded by a Split Over Israel',\n",
       " 'Second City Expands to the First City and Sets Up Shop in Brooklyn',\n",
       " 'FirstEnergy Ex-C.E.O. and 2 Others Are Indicted in Bribery Scandal',\n",
       " 'Tuesday Briefing: Israel Strikes Gaza to Rescue Hostages',\n",
       " 'The Chiefs Won the Super Bowl. Will Taylor Swift Visit the White House?',\n",
       " 'Trump Backs His Candidate for R.N.C. Chair, and His Daughter-in-Law for Co-Chair',\n",
       " 'Love or Hate Valentine’s Day?',\n",
       " 'U.S., U.N. and International Criminal Court Intensify Warnings Against Invading Rafah',\n",
       " 'Bob Edwards, Longtime Host of NPR’s ‘Morning Edition,’ Dies at 76',\n",
       " 'Trump’s Legal Cases: Here, There and Everywhere',\n",
       " 'Toby Keith Returns to the Top of the Billboard Album Chart',\n",
       " 'Senate Passes Aid to Ukraine, but Fate Is Uncertain in a Hostile House',\n",
       " 'Here Are the Republicans Who Broke With Their Party to Back Ukraine Aid',\n",
       " 'Review: In ‘Self Portraits (Deluxe),’ a Provocateur Instigates Reflection',\n",
       " 'Pelosi’s Hometown Paper Backs Her, With an Eye on the Calendar',\n",
       " 'Austin Cancels Trip to Europe After Returning to Hospital',\n",
       " 'Sunken Ship Discovered in Lake Superior',\n",
       " 'A Shipwreck Is Found in Lake Superior. Its Captain’s Behavior Remains a Mystery.',\n",
       " 'Israeli Raid in Rafah Rescues 2 Hostages and Kills Dozens, Officials Say',\n",
       " 'An Israeli Raid Rescued Two Hostages and Killed Dozens, Officials Say',\n",
       " 'Children Born to Mothers With Pregnancy Complications Face Higher Heart Risks',\n",
       " 'One Killed and 5 Wounded in Shooting at Bronx Subway Station, Police Say',\n",
       " 'How Special Is New York’s Special Election?',\n",
       " 'Trump Attends Court Hearing on Access to Classified Documents',\n",
       " 'Defending Troops, Haley Says Golf Course Is Closest Trump Has Come to Combat']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_article_list(\"2024-02-12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b82c6e",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\"></a>\n",
    "#### Task 2\n",
    "Write some code that explores whether the fields \"abstract\" and \"snippet\" are always the same or they differ. Which one has more information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "985854e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 0\n",
      "188 0\n",
      "302 0\n",
      "193 0\n",
      "270 0\n",
      "232 0\n",
      "288 250\n",
      "423 0\n",
      "154 0\n",
      "253 250\n",
      "519 0\n",
      "182 0\n",
      "353 0\n",
      "278 0\n",
      "342 250\n",
      "253 250\n",
      "213 0\n",
      "239 0\n",
      "114 0\n",
      "249 250\n",
      "244 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21, 0, 3770)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_longer = 0\n",
    "snippet_longer = 0\n",
    "equal = 0\n",
    "\n",
    "for article in articles['response']['docs']:\n",
    "    if article['abstract'] > article['snippet']:\n",
    "        abstract_longer = abstract_longer+1\n",
    "        print(len(article['abstract']), len(article['snippet']))\n",
    "    elif article['abstract'] < article['snippet']:\n",
    "        snippet_longer = snippet_longer + 1\n",
    "    else:\n",
    "        equal = equal + 1\n",
    "\n",
    "(abstract_longer, snippet_longer, equal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483baaa",
   "metadata": {},
   "source": [
    "From this exploration, it appears that the abstract and snippet are usually the same. However, in a few cases, the abstract is longer than the snippet, often because the snippet is non-existent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90397d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3\"></a>\n",
    "#### Task 3\n",
    "Write a function that given one article (in its nested structure), creates a flat dictionary with keys that are relevant for analysis: either the abstract or snippet (see point 2); lead paragraph; headline; keywords concatenated via semicolon; pub_date; document_type; section_name; and type_of_material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aed4ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_article(article_dict):\n",
    "    article_dict_flat = {}\n",
    "    article_dict_flat['abstract'] = article_dict['abstract']\n",
    "    article_dict_flat['lead_paragraph'] = article_dict['lead_paragraph']\n",
    "    article_dict_flat['headline'] = article_dict['headline']['main']\n",
    "    article_dict_flat['keywords'] = \"; \".join(keyword['value'] for keyword in article_dict['keywords'] if isinstance(keyword['value'], str))\n",
    "    article_dict_flat['pub_date'] = article_dict['pub_date'] # technically has date and time\n",
    "    article_dict_flat['document_type'] = article_dict['document_type']\n",
    "    article_dict_flat['section_name'] = article_dict['section_name']\n",
    "    article_dict_flat['type_of_material'] = article_dict['type_of_material']\n",
    "    return article_dict_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63372f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'Periods of backlash take shape after surges of Black progress. We have entered another such period.',\n",
       " 'lead_paragraph': 'I am fascinated, and alarmed, by the swiftness with which periods of backlash take shape after surges of Black progress, and I believe that we have entered another such period.',\n",
       " 'headline': 'The Dawn of a New Era of Oppression',\n",
       " 'keywords': 'Hate Crimes; Black People; Blacks; Discrimination; Civil Rights Movement (1954-68); Reconstruction Era; Segregation and Desegregation',\n",
       " 'pub_date': '2024-02-01T00:00:08+0000',\n",
       " 'document_type': 'article',\n",
       " 'section_name': 'Opinion',\n",
       " 'type_of_material': 'Op-Ed'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_article(articles['response']['docs'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02d0c1",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\"></a>\n",
    "#### Task 4\n",
    "Write another function that calls the function from point 3 on every article, to create a list of article dictionaries, and convert this list into a dataframe and then store it as a CSV file with the date-month in the title (this is important for point 5 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a3b67e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_article_csv(date):\n",
    "    \"\"\"\n",
    "    Many parts similar to daily_article_list()\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    print(f\"Searching for NYT articles from {date}\")\n",
    "\n",
    "    year = date[:4]\n",
    "    month = date[5:7].strip('0')\n",
    "    day = date[8:10]\n",
    "\n",
    "    month_json_want = f\"NYT_{year}-{month}.json\"\n",
    "    month_json_current = [file for file in os.listdir(cwd) if file.endswith(\".json\")] # check if that month's json is in the folder\n",
    "\n",
    "    if month_json_want not in month_json_current: # if that month's json is not in the folder, get it\n",
    "        print(f\"Getting articles from {month}-{year}\")\n",
    "        getNYTArticles(year, month, myAPIkey)\n",
    "    else:\n",
    "        print(f\"Already have articles from {month}-{year}\")\n",
    "\n",
    "    with open(os.path.join(cwd,month_json_want)) as inf:\n",
    "        articles = json.load(inf)\n",
    "    \n",
    "    daily_article_dict_list = [flatten_article(article) for article in articles['response']['docs'] if article['pub_date'][:10] == date]\n",
    "    daily_article_df = pd.DataFrame(daily_article_dict_list)\n",
    "\n",
    "    daily_article_df.to_csv(f\"NYT_{date}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7e56c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for NYT articles from 2024-02-12\n",
      "Already have articles from 2-2024\n"
     ]
    }
   ],
   "source": [
    "daily_article_csv(\"2024-02-12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3133901",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5\"></a>\n",
    "#### Task 5\n",
    "Once you have done all of these in the notebook, create a Python script that can be called with a date (from a TikTok video). First, the script looks whether a CSV with cleaned articles is in our folder. If not, calls first the API function to get the articles and then the function that converts them into a CSV. Then, it loads the CSV into a datafram and it uses filtering to get the articles for the desired date. These articles will be used for the Semantic Similarity portion of the TikTok Project.\n",
    "\n",
    "Done in \"nyt_get_articles_script.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061baf0d-6c54-457b-b3d7-aecd0fd29b10",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## Part 2: Cosine Similarity for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ba378-a8b9-44c2-806c-d46672d67d12",
   "metadata": {},
   "source": [
    "We will start with the example that was in the slides. There, we initially used the Jaccard similarity to rank sentences most similar to a query, and when that didn't work as expected, we looked at the cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02ccfc-4467-4bc5-9361-6e934c24c4dc",
   "metadata": {},
   "source": [
    "### Use Jaccard similarity for a query phrase and a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2df6d-775c-4c58-b3d4-d72ae5ee94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"red dress\"\n",
    "\n",
    "sentences = [\n",
    "\"she wore a dress and red earrings\",\n",
    "\"the dress has a red wine stain\",\n",
    "\"tomorrow I will wear my new red dress\",\n",
    "\"the red dress in the photo resembles the red dress she is wearing\",\n",
    "\"short dress\",\n",
    "\"red lipstick\"\n",
    "]\n",
    "\n",
    "def jaccard(text1, text2):\n",
    "    \"\"\"Implement Jaccard similarity. Assumes there is no punctuation in text.\"\"\"\n",
    "    sw1 = set(text1.lower().split()) # turn into a set of words\n",
    "    sw2 = set(text2.lower().split())\n",
    "    sim = len(sw1.intersection(sw2)) / len(sw1.union(sw2))\n",
    "    return round(sim, 4) # round to 4 digits after the comma\n",
    "\n",
    "def applyJaccard(query, sentences):\n",
    "    \"\"\"Appl the Jaccard similarity between query and each sentence\"\"\"\n",
    "    results = []\n",
    "    for sent in sentences:\n",
    "        jac = jaccard(query, sent)\n",
    "        results.append((jac, sent))\n",
    "    \n",
    "        # Sort in descending order\n",
    "        results.sort(reverse=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "# call the function\n",
    "\n",
    "applyJaccard(q, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a902535-3e2f-40cb-b1e1-83f12d9cfbdd",
   "metadata": {},
   "source": [
    "As we discussed in class, the Jaccard similarity is not doing well with our data (showing as similar text that, thus, we will try the cosine similarity. However, in order to apply the cosine similarity, we need some other steps:\n",
    "\n",
    "1. Create the vocabulary of words that will serve as the dimensions of our vector space\n",
    "2. Represent each document as a vector in the vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b7baa-d3e9-4ab5-936b-d157ceb4ecaa",
   "metadata": {},
   "source": [
    "### Create Vocabulary\n",
    "\n",
    "While our sentences in the example don't have punctuation, most of the time text will have it, thus, we need to be prepared to remove it. This will be necesary in order to avoid a word show multiple times, with and without punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eada87-b484-4ff6-86ae-8e3da3b44f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"that, that is the thing I want: dancing by the river! ah, the river, I have missed it so much!\"\n",
    "phrase.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10a224-8a04-4cf6-902f-5be5bc39acbc",
   "metadata": {},
   "source": [
    "Notice how we have both \"that!\" and \"that\", and also \"river,\" and \"river!\". This is why we will remove punctuation. Luckily, Python has a library that lists all punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a24b4-dbed-47a5-aef3-6932ae997af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a72c-0d0a-4b72-9dfc-da57935f5cd4",
   "metadata": {},
   "source": [
    "One way to go about it is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbb783-deea-4385-8133-3ba596115a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(char for char in phrase if char not in string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3885419-d1b4-470f-b877-7b352eb43522",
   "metadata": {},
   "source": [
    "Notice how all the punctuation is gone. Now that we know how to do this, we can write our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b23d86-8bd4-48a6-96b8-5acea2294f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(textchunk):\n",
    "    \"\"\"Given some text, create the vocabulary of unique words.\"\"\"\n",
    "    textchunk = textchunk.lower()\n",
    "    cleantext = \"\".join(char for char in textchunk if char not in string.punctuation)\n",
    "    words = set(cleantext.split())\n",
    "    voc = sorted(words)\n",
    "\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec90fd-b1d9-41b5-bf1d-b0ff75d8576c",
   "metadata": {},
   "source": [
    "Let's test it with our sentences. Since they are a list, we turn them into a string first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8530bd-7e6c-423b-a6c1-9d8a5a3d3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "getVocabulary(\" \".join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aa541-20e6-4649-a072-e7747c8c0e7c",
   "metadata": {},
   "source": [
    "It looks good, no word is repeated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b4d6b-ed75-46dd-8eb7-cdc086929acb",
   "metadata": {},
   "source": [
    "### Vector representation\n",
    "\n",
    "Now that we have a vocabulary, we can easily convert every sentence into a vector of numbers. Remember, all the vectors will have the same length. They will have 0 for a dimension (word) that they don't have, and the count of word for a dimension they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df263f81-89fe-43e0-b8c2-c3a7389cbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vector(sentence, voc):\n",
    "    \"\"\"Given a sentence and the vocabulary for the problem,\n",
    "    turn every sentence into a vector.\n",
    "    \"\"\"\n",
    "    cleantext = \"\".join(char for char in sentence if char not in string.punctuation)\n",
    "    words = cleantext.lower().split()\n",
    "    vector = [words.count(w) for w in voc]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373e862-7388-441f-be8c-df4858a5049d",
   "metadata": {},
   "source": [
    "Let's try it with one sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41a829-15bb-4cfb-87b6-71fe59f1eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = getVocabulary(\" \".join(sentences))\n",
    "text2vector(sentences[0], voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3160b6-9859-4232-9633-8c7c9d1554df",
   "metadata": {},
   "source": [
    "Let's verify that this is done right by checking what sentence was turned into a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0bd5c3-d39d-45df-b299-d68985f95e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283598ad-4b39-4971-8678-07e9e58a5fe8",
   "metadata": {},
   "source": [
    "Let's combine the vocabulary and the vector to see the pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07681d03-9145-4d0d-bc62-35d8e7d08091",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(voc, text2vector(sentences[0], voc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044674c-702e-41cd-acf7-a256ad70319b",
   "metadata": {},
   "source": [
    "Notice how each word in our sentence has a 1 next to it and all the other words have a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324cbe7f-5532-4400-9072-e5be860c9c29",
   "metadata": {},
   "source": [
    "We will now convert all the sentences to vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f4537-e117-4a85-9abc-9f7d5a0f232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2vec = [text2vector(sent, voc) for sent in sentences]\n",
    "sent2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d6ada-dfdd-46c2-819c-78eeef345a0c",
   "metadata": {},
   "source": [
    "We represent this in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80266fcd-74f1-425c-b9f7-7d5cf5e2fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sent2vec, \n",
    "                  columns=voc,\n",
    "                  index=[f\"doc_{i+1}\" for i in range(len(sentences))])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb0b24-529d-4729-9536-07133eed7954",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "We discussed the implementation of cosine similarity in class. Below is the function that implements it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56ea1c-ad57-4932-897f-853683b8dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    "def cosineSimilarity(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    V1 = np.array(vec1)\n",
    "    V2 = np.array(vec2)\n",
    "    cosine = np.dot(V1, V2)/(norm(V1)*norm(V2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d180583c-772e-417d-9bb3-5a757cd4b02a",
   "metadata": {},
   "source": [
    "Now that we have the cosine similarity function, we will write a function that given a query and a list of sentences, calculates the similarity score for each pair (query, sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84fcec-d7b6-4778-b325-f71186be5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(query, sentences):\n",
    "    \"\"\"Given a query and some sentences, rank the sentences for \n",
    "    which are the most similar to the query.\n",
    "    \"\"\"\n",
    "    # Step 1: create vocabulary\n",
    "    voc = getVocabulary(\" \".join(sentences))\n",
    "\n",
    "    # Step 2: generate vector for query\n",
    "    queryVec = text2vector(query, voc)\n",
    "\n",
    "    # Step 3: generate vector for sentences and calculate cosine similarity at once\n",
    "    similarities = []\n",
    "    for sent in sentences:\n",
    "        sentVec = text2vector(sent, voc)\n",
    "        sim = cosineSimilarity(queryVec, sentVec)\n",
    "        similarities.append((round(sim, 4), sent)) # keep track of sentences\n",
    "\n",
    "    similarities.sort(reverse=True) # most similar sentence at the top\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34727669-66b5-457a-bd42-179a69b7e0dd",
   "metadata": {},
   "source": [
    "Now we can call the function for our query \"red dress\" and the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2c623-8e46-41c2-a5c8-44f16ef214a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankDocuments(\"red dress\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f57d06-5e24-499b-ba9d-71206812ee93",
   "metadata": {},
   "source": [
    "**Note:** These values are slightly different from the ones in the slides. There was a bug with the word \"I\", which was not lowercased in the sentences, so it didn't count in the vector. The bug has been fixed in this version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31052f-a095-4a3b-963d-93ea3d8aa447",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>\n",
    "## Part 3: Similarity of Spring and Summer sentences\n",
    "\n",
    "You were given the following sentences in the slides of Day 10. These were created by GenAI to capture the spirit of \"spring\" and \"summer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac7add-0cab-4e15-8c8c-670a22bd24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "springSentences = [\n",
    "\"As spring unfolds, the warmth of the season encourages the first blossoms to open, signaling longer days ahead.\",\n",
    "\"Spring brings not only blooming flowers but also the anticipation of sunny days and outdoor activities.\",\n",
    "\"With the arrival of spring, people begin planning their summer vacations, eager to enjoy the seasonal warmth.\",\n",
    "\"The mild spring weather marks the transition from the cold winter to the inviting warmth of summer.\",\n",
    "\"During spring, families often start spending more time outdoors, enjoying the season's pleasant temperatures and the promise of summer fun.\"\n",
    "]\n",
    "\n",
    "summerSentences = [\n",
    "\"Summer continues the season's trend of growth and warmth, with gardens full of life and days filled with sunlight.\",\n",
    "\"The summer season is synonymous with outdoor adventures and enjoying the extended daylight hours that began in spring.\",\n",
    "\"As summer arrives, the warm weather invites a continuation of the outdoor activities that people began enjoying in spring.\",\n",
    "\"The transition into summer brings even warmer temperatures, allowing for beach visits and swimming, much awaited since the spring.\",\n",
    "\"Summer vacations are often planned as the days grow longer, a pattern that starts in the spring, culminating in peak summer leisure.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d7cfd-7d88-4ae0-a4d8-fd57d7abf98f",
   "metadata": {},
   "source": [
    "**Our Goal:**\n",
    "\n",
    "We want to generate a heatmap of the similarity scores between all sentences to one another to find out how similar they are. To achieve this goal, we need to break down the task:\n",
    "\n",
    "1. We need to create first the vocabulary of all terms (or the dimensions of our vector space).\n",
    "2. We will turn every sentence into a vector.\n",
    "3. We will compare every sentence to every other sentence through the cosine similartiy to create the similarity matrix.\n",
    "4. We will draw the heatmap with seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d4566-ff9e-440f-8270-28266576f1d4",
   "metadata": {},
   "source": [
    "### Create Vocabulary\n",
    "\n",
    "We will call the function `getVocabulary` that we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76cc83-44a6-483a-8b62-504aa457b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "allSentences = \" \".join(springSentences) + \" \" + \" \".join(summerSentences)\n",
    "voc = getVocabulary(allSentences)\n",
    "print(f\"Vocabulary has {len(voc)} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e8c8e-3ba6-4186-888c-419c40f4371d",
   "metadata": {},
   "source": [
    "### Convert sentences to vectors\n",
    "\n",
    "We will call the function `text2vector` on every sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cb611-dbc2-4758-949d-8106223faa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentVectors = [text2vector(sent, voc) for sent in springSentences+summerSentences]\n",
    "print(len(sentVectors), len(sentVectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c7b5a-af95-4c18-8055-2595ad5c325c",
   "metadata": {},
   "source": [
    "This means that we created 10 vectors, each with a length of 102 dimensions.  \n",
    "Let's check our work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf70219-42ae-470d-a1ee-ed77b5f6df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneSent = springSentences[0]\n",
    "oneSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588ad7d-9c4f-4fb5-8c5b-92d9701e3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(zip(text2vector(oneSent, voc), voc))\n",
    "nonZero = [pair for pair in pairs if pair[0] != 0]\n",
    "nonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0ed94-87d4-4709-ba59-9554e5ff6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Words in sentence: {len(oneSent.split())}; nonzero terms in vector: {len(nonZero)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0a4c2-acfb-40d6-b15e-dda74d2f5f12",
   "metadata": {},
   "source": [
    "This looks good. There are 16 unique words, and the word \"the\" is repeated two more times, that explains the numbers 16 and 18. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c5533-86e8-4829-9de5-e3e8ac0d8cd0",
   "metadata": {},
   "source": [
    "### Calculating the similarity matrix\n",
    "\n",
    "We will calculate the cosine similarity for every pair of sentences. This makes sense because we only have 10 sentences, if we had way more, we will try to be more efficient and not repeat the calculations (since we know that the matrix is symmetrical). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b9b81-d5b1-43f2-b029-1b15a78014e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "simMatrix = []\n",
    "for vec1 in sentVectors:\n",
    "    simRow = []\n",
    "    for vec2 in sentVectors:\n",
    "        simRow.append(cosineSimilarity(vec1, vec2))\n",
    "    simMatrix.append(simRow)\n",
    "\n",
    "print(simMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee2c37-ac61-41de-be5c-1c4d0ed65905",
   "metadata": {},
   "source": [
    "### Generate the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15937a9a-5ed7-4f45-b2a9-8fa4d9b15f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def drawHeatmap(sentLabels, simMtrx, plotTitle):\n",
    "    \"\"\"Draws a heatmap for the similarity matrix.\n",
    "    \"\"\"\n",
    "    sns.set(font_scale=0.9)\n",
    "    g = sns.heatmap(\n",
    "          simMtrx, # similarity matrix with the cosine sim values\n",
    "          xticklabels=sentLabels,\n",
    "          yticklabels=sentLabels,\n",
    "          vmin=0,\n",
    "          vmax=1,\n",
    "          cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(sentLabels, rotation=90)\n",
    "    g.set_title(plotTitle, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0d895-0cbc-46e0-abc4-039dac2e69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortSent = [sent[:25] for sent in springSentences+summerSentences]\n",
    "drawHeatmap(shortSent, simMatrix, \"Cosine similarity matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb27fe5-0ad2-4e5d-b3ea-d50ae4d1fccb",
   "metadata": {},
   "source": [
    "### Short Exploration\n",
    "\n",
    "Let's look at the similarity matrix in a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a402d1-8fd8-439c-85b5-2ec2524cb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"s{i+1}\" for i in range(10)]\n",
    "df = pd.DataFrame(simMatrix, columns=labels, index=labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db3f119-4464-43e2-ac5f-351e6526db01",
   "metadata": {},
   "source": [
    "I will write some code to compare sentences that have a high similarity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081a313-7a0b-4504-9077-cc8af1d2d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(sent):\n",
    "    \"\"\"Get the words of a sentence after lowercasing and removing punctuation.\"\"\"\n",
    "    cleantext = \"\".join(char for char in sent.lower() if char not in string.punctuation)\n",
    "    cleanWords = cleantext.split()\n",
    "    return cleanWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5258d-495a-4ba3-b498-65b080bcf912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareSentences(sent1, sent2):\n",
    "    \"\"\"Compare the content of two sentences.\"\"\"\n",
    "    words1 = getWords(sent1)\n",
    "    words2 = getWords(sent2)\n",
    "    commonWords = sorted([w for w in words1 if w in words2])\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"Sent1: \", sent1)\n",
    "    print(\"Sent2: \", sent2)\n",
    "    print(f\"Lengths of sentences: {len(words1)} and {len(words2)}. Words in common: {len(commonWords)}\")\n",
    "    print(\"Common words:\", commonWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021a92a-4bb4-4fc8-8689-4b13e9f31307",
   "metadata": {},
   "source": [
    "Let's check s1 and s4, in the group os Spring sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31164c1-f33b-4be3-a43b-ec5eb5c2bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareSentences(springSentences[0], springSentences[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fab4a1-d4f6-4572-bc2d-23814324f8c1",
   "metadata": {},
   "source": [
    "What about the sentences s7 and s8, in the group of Summer sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7e464-6db4-4e99-9b17-77b911ffb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareSentences(summerSentences[1], summerSentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6a16c-48d8-48ab-bfd2-eeadbaf356ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
