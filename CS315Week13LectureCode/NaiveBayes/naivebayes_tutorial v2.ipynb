{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589c1e17",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Tutorial for the Naive Bayes classifier using scikit-learn. This example uses Pyktok data to classify TikTok videos as ads or non-ads.\n",
    "\n",
    "Code based on tutorial from StackAbuse: https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/  \n",
    "\n",
    "### 1. Preparing our data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5de966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pyktok_ad_data.csv',\n",
    "                   usecols=['video_id', 'suggested_words', 'video_description', 'video_is_ad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360685f",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbffc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['video_is_ad'] = df.video_is_ad.map({False: 0, True: 1})\n",
    "\n",
    "#merge the suggested_words and video_description columns\n",
    "df['description'] = df['suggested_words'].combine_first(df['video_description'])\n",
    "\n",
    "#lowercase and remove punctuation\n",
    "df['description'] = df.description.map(lambda x: x.lower())\n",
    "df['description'] = df.description.str.replace('[^\\w\\s]', '')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc848c1-0e9c-49ca-b370-e049a6c5baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d49754",
   "metadata": {},
   "source": [
    "#### Tokenize the descriptions into separate words using nltk\n",
    "\n",
    "You will need to install the nltk library, if you don't have it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e22a47-ee96-4ea0-a915-e74bc7832be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d942be",
   "metadata": {},
   "source": [
    "***NOTE:***\n",
    "The code below will open a dialog window to ask you to downlaod some packages. In that window, switch to the \"Models\" tab and choose \"punkt\" from the \"Identifier\" column. Click \"Download\" and it will install the necessary files to apply tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ee750",
   "metadata": {},
   "source": [
    "#### Perform word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82851282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "df['description'] = df['description'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1575381",
   "metadata": {},
   "source": [
    "#### Use CountVectorizer to transform data into occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09500d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# This converts the list of words into space-separated strings\n",
    "df['description'] = df['description'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(df['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4570a22",
   "metadata": {},
   "source": [
    "#### Use TF-IDF as model features instead of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "\n",
    "counts = transformer.transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59564c8d",
   "metadata": {},
   "source": [
    "### 2. Using the Naive Bayes Model\n",
    "\n",
    "#### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "shuffled_df = df.sample(frac=1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, shuffled_df['video_is_ad'], \n",
    "                                                    test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768ccf6",
   "metadata": {},
   "source": [
    "#### Fit the data to a Naive Bayes classifier.\n",
    "\n",
    "We use the Multinomial Naive Bayes Classifier here for text classification. There are other types of Naive Bayes classifiers for a variety of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee3a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837eddec",
   "metadata": {},
   "source": [
    "#### Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d36282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80dc5e0",
   "metadata": {},
   "source": [
    "Our model's accuracy varies between 60-75%, which isn't great...Let's check the number of features and the sparsity of the document-term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = len(count_vect.get_feature_names_out())\n",
    "print(\"Number of features:\", features)\n",
    "\n",
    "#Sparsity is the number of zero-valued elements divided by the total number of elements\n",
    "sparsity = (1- np.count_nonzero(X_train.toarray()) / np.prod(X_train.shape)) * 100\n",
    "print(\"Sparsity:\", sparsity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca66f65",
   "metadata": {},
   "source": [
    "We can use a confusion matrix to get a better idea of our model's performance:\n",
    "\n",
    "### 3. Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52559fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot confusion matrix\n",
    "ax = sns.heatmap(conf_matrix, annot=True)\n",
    " \n",
    "# set x-axis label and ticks. \n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14, labelpad=20)\n",
    "ax.xaxis.set_ticklabels(['Non-Ad', 'Ad'])\n",
    " \n",
    "# set y-axis label and ticks\n",
    "ax.set_ylabel(\"True label\", fontsize=14, labelpad=20)\n",
    "ax.yaxis.set_ticklabels(['Non-Ad', 'Ad'])\n",
    " \n",
    "# set plot title\n",
    "ax.set_title(\"Confusion Matrix for TikTok Ad Detection Model\", fontsize=14, pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out the values for each cell in the confusion matrix:\n",
    "true_neg, false_pos, false_neg, true_pos = conf_matrix.ravel()\n",
    " \n",
    "true_neg, false_pos, false_neg, true_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ca373-6432-4c28-821c-54ffa319761c",
   "metadata": {},
   "source": [
    "**Calculate f1_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bac9a-378e-4d74-b271-e3a17e8a4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test.values, predicted, average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
